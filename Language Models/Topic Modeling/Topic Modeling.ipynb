{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa27cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/zx8bq5k10yj_k1w7vcfbvwr00000gn/T/ipykernel_54598/3961290547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparse2Corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from bertopic import BERTopic\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4e2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Downloading bertopic-0.16.2-py2.py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdbscan>=0.8.29\n",
      "  Downloading hdbscan-0.8.36.tar.gz (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentence-transformers>=0.4.1\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[K     |████████████████████████████████| 227 kB 45.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting umap-learn>=0.5.0\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from bertopic) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from bertopic) (4.62.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from bertopic) (5.13.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from bertopic) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from bertopic) (1.2.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
      "Collecting cython<3,>=0.27\n",
      "  Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from hdbscan>=0.8.29->bertopic) (1.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.1.5->bertopic) (2021.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.34.0\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.1 MB 53.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch>=1.11.0\n",
      "  Downloading torch-2.2.2-cp38-none-macosx_10_9_x86_64.whl (150.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 150.6 MB 37.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.15.1\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[K     |████████████████████████████████| 402 kB 38.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (5.4.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[K     |████████████████████████████████| 176 kB 45.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.9.0)\n",
      "Requirement already satisfied: requests in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.26.0)\n",
      "Requirement already satisfied: filelock in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.4.7)\n",
      "Requirement already satisfied: jinja2 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 34.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 43.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp38-cp38-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 62.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp38-cp38-macosx_10_12_x86_64.whl (416 kB)\n",
      "\u001b[K     |████████████████████████████████| 416 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (2023.3.23)\n",
      "Collecting pynndescent>=0.5\n",
      "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba>=0.51.2\n",
      "  Downloading numba-0.58.1-cp38-cp38-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 19.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (7.0.1)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0\n",
      "  Downloading llvmlite-0.41.1-cp38-cp38-macosx_10_9_x86_64.whl (31.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.0 MB 46.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata->numba>=0.51.2->umap-learn>=0.5.0->bertopic) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2024.2.2)\n",
      "Collecting mpmath<1.4.0,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 58.3 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (PEP 517) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /Users/Neelay/opt/anaconda3/bin/python /Users/Neelay/opt/anaconda3/lib/python3.8/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/mj/zx8bq5k10yj_k1w7vcfbvwr00000gn/T/tmpo2ezf_ta\n",
      "       cwd: /private/var/folders/mj/zx8bq5k10yj_k1w7vcfbvwr00000gn/T/pip-install-z72g10ny/hdbscan_7cd750ae04c2403f9c79c7f4795c42fc\n",
      "  Complete output (37 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-x86_64-cpython-38\n",
      "  creating build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/validity.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/flat.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/prediction.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/plots.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/hdbscan_.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  copying hdbscan/robust_single_linkage_.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  creating build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  copying hdbscan/tests/test_flat.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  copying hdbscan/tests/test_prediction_utils.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  copying hdbscan/tests/__init__.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  copying hdbscan/tests/test_rsl.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  copying hdbscan/tests/test_hdbscan.py -> build/lib.macosx-10.9-x86_64-cpython-38/hdbscan/tests\n",
      "  running build_ext\n",
      "  skipping 'hdbscan/_hdbscan_tree.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan/_hdbscan_linkage.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan/_hdbscan_boruvka.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan/_hdbscan_reachability.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan/_prediction_utils.c' Cython extension (up-to-date)\n",
      "  skipping 'hdbscan/dist_metrics.c' Cython extension (up-to-date)\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  creating build/temp.macosx-10.9-x86_64-cpython-38\n",
      "  creating build/temp.macosx-10.9-x86_64-cpython-38/hdbscan\n",
      "  x86_64-apple-darwin13.4.0-clang -fno-strict-aliasing -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O3 -Wall -Wstrict-prototypes -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -pipe -fdebug-prefix-map=${SRC_DIR}=/usr/local/src/conda/${PKG_NAME}-${PKG_VERSION} -fdebug-prefix-map=/Users/Neelay/opt/anaconda3=/usr/local/src/conda-prefix -flto -Wl,-export_dynamic -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O3 -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -isystem /Users/Neelay/opt/anaconda3/include -D_FORTIFY_SOURCE=2 -isystem /Users/Neelay/opt/anaconda3/include -I/Users/Neelay/opt/anaconda3/include/python3.8 -I/private/var/folders/mj/zx8bq5k10yj_k1w7vcfbvwr00000gn/T/pip-build-env-sspqb04c/overlay/lib/python3.8/site-packages/numpy/core/include -c hdbscan/_hdbscan_tree.c -o build/temp.macosx-10.9-x86_64-cpython-38/hdbscan/_hdbscan_tree.o\n",
      "  clang-12: warning: -Wl,-export_dynamic: 'linker' input unused [-Wunused-command-line-argument]\n",
      "  In file included from hdbscan/_hdbscan_tree.c:6:\n",
      "  /Users/Neelay/opt/anaconda3/include/python3.8/Python.h:25:10: fatal error: 'stdio.h' file not found\n",
      "  #include <stdio.h>\n",
      "           ^~~~~~~~~\n",
      "  1 error generated.\n",
      "  error: command '/Users/Neelay/opt/anaconda3/bin/x86_64-apple-darwin13.4.0-clang' failed with exit code 1\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for hdbscan\u001b[0m\n",
      "\u001b[?25hFailed to build hdbscan\n",
      "\u001b[31mERROR: Could not build wheels for hdbscan which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff427db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "documents = newsgroups_train.data\n",
    "labels = newsgroups_train.target\n",
    "label_names = newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "# Dataset summary\n",
    "print(\"Number of documents:\", len(documents))\n",
    "print(\"Number of categories:\", len(label_names))\n",
    "print(\"Categories:\", label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7754277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categories\n",
    "category_counts = pd.Series(labels).value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values, palette=\"viridis\")\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Distribution of Categories in 20 Newsgroups Dataset')\n",
    "plt.xticks(ticks=np.arange(len(label_names)), labels=label_names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4dbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document length analysis\n",
    "document_lengths = [len(doc.split()) for doc in documents]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(document_lengths, bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Document Length (words)')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3229c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fda9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035693fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (Gensim)\n",
    "# Explanation: LDA (Latent Dirichlet Allocation) is a generative probabilistic model that assumes each document is a mixture of topics and each topic is a mixture of words.\n",
    "corpus = Sparse2Corpus(X, documents_columns=False)\n",
    "id2word = Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in vectorizer.vocabulary_.items()))\n",
    "lda_model_gensim = LdaModel(corpus, num_topics=10, id2word=id2word, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (scikit-learn)\n",
    "# Explanation: Another implementation of LDA using scikit-learn which follows the same probabilistic approach.\n",
    "lda_model_sklearn = LatentDirichletAllocation(n_components=10, random_state=0)\n",
    "lda_topics_sklearn = lda_model_sklearn.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMF\n",
    "# Explanation: NMF (Non-negative Matrix Factorization) is a linear algebra technique that factorizes the document-term matrix into non-negative matrices. It’s useful for parts-based representation.\n",
    "nmf_model = NMF(n_components=10, random_state=0)\n",
    "nmf_topics = nmf_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA (TruncatedSVD)\n",
    "# Explanation: LSA (Latent Semantic Analysis) uses SVD (Singular Value Decomposition) to reduce the dimensionality of the document-term matrix, capturing the underlying structure in the data.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "lsa_model = TruncatedSVD(n_components=10, random_state=0)\n",
    "lsa_topics = lsa_model.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic\n",
    "# Explanation: BERTopic leverages BERT embeddings and clustering algorithms to identify topics in text data. It captures nuanced meanings in documents due to advanced contextual embeddings.\n",
    "bertopic_model = BERTopic(language=\"english\")\n",
    "bertopic_topics, probs = bertopic_model.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c6669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (Coherence Score for LDA)\n",
    "# Explanation: Coherence score measures the quality of the topics. Higher coherence scores indicate more interpretable topics.\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_gensim, texts=[doc.split() for doc in documents], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation (Coherence Score for BERTopic)\n",
    "topic_words = [topic[1] for topic in bertopic_model.get_topics().values()]\n",
    "coherence_model_bertopic = CoherenceModel(topics=topic_words, texts=[doc.split() for doc in documents], coherence='c_v')\n",
    "coherence_bertopic = coherence_model_bertopic.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ce37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top words for each topic for each model\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic #{topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "print(\"LDA Topics (scikit-learn):\")\n",
    "print_top_words(lda_model_sklearn, vectorizer.get_feature_names_out(), n_top_words)\n",
    "\n",
    "print(\"NMF Topics:\")\n",
    "print_top_words(nmf_model, vectorizer.get_feature_names_out(), n_top_words)\n",
    "\n",
    "print(\"LSA Topics:\")\n",
    "print_top_words(lsa_model, tfidf_vectorizer.get_feature_names_out(), n_top_words)\n",
    "\n",
    "print(\"BERTopic Topics:\")\n",
    "print(bertopic_model.get_topic_info())\n",
    "\n",
    "print(f\"LDA Coherence Score (Gensim): {coherence_lda}\")\n",
    "print(f\"BERTopic Coherence Score: {coherence_bertopic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LDA topics using pyLDAvis\n",
    "lda_vis = gensimvis.prepare(lda_model_gensim, corpus, id2word)\n",
    "pyLDAvis.save_html(lda_vis, 'lda_gensim.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for top words in each model\n",
    "def plot_wordcloud(model, feature_names, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    wordcloud = WordCloud(width=800, height=400, max_words=50, colormap='viridis').generate_from_frequencies(dict(zip(feature_names, model.components_.flatten())))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ed934",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(lda_model_sklearn, vectorizer.get_feature_names_out(), \"Wordcloud for LDA (scikit-learn)\")\n",
    "plot_wordcloud(nmf_model, vectorizer.get_feature_names_out(), \"Wordcloud for NMF\")\n",
    "plot_wordcloud(lsa_model, tfidf_vectorizer.get_feature_names_out(), \"Wordcloud for LSA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTopic visualization\n",
    "fig = bertopic_model.visualize_topics()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of coherence scores\n",
    "model_names = ['LDA (Gensim)', 'BERTopic']\n",
    "coherence_scores = [coherence_lda, coherence_bertopic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6bb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=model_names, y=coherence_scores, palette='viridis')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.title('Coherence Scores of Topic Models')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
